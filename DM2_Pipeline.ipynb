{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DM2_Pipeline.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX3qG4zQ9_-y",
        "colab_type": "text"
      },
      "source": [
        "# 2019 VU Data Mining Techniques Cup\n",
        "\n",
        "link: https://www.kaggle.com/t/81fd4b6b248c4642930d5c1013af967a\n",
        "\n",
        "__TASK__: \"Predict what hotels properties listed as a result of a hotel search a user is most likely to click on.\"\n",
        "\n",
        " __Evaluation metric__: Normalized Discounted Cumulative Gain (NDCG).\n",
        " \n",
        " \n",
        "### DATASET\n",
        "\n",
        "For a nice overview on the data fields of the task,  refer to https://www.kaggle.com/c/expedia-personalized-sort/data ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASkqt6kSb3cU",
        "colab_type": "text"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PejfipdtcBh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Install dependencies\n",
        "!pip install PyDrive\n",
        "!pip install kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-3ayC1Eb3cW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import sys\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from scipy import stats\n",
        "from google.colab import files\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "import xgboost\n",
        "from sklearn import preprocessing\n",
        "import json\n",
        "import pandas as pd\n",
        "from pandas.api.types import is_numeric_dtype\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import copy \n",
        "import datetime\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "\n",
        "pd.set_option('display.max_column',None)\n",
        "pd.set_option('display.max_rows',None)\n",
        "pd.set_option('display.max_seq_items',None)\n",
        "pd.set_option('display.max_colwidth', 500)\n",
        "pd.set_option('expand_frame_repr', True)\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx56RD3kcHp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if \"training_set_VU_DM.csv\" not in os.listdir():\n",
        "    !mkdir .kaggle\n",
        "    # Create API Token for my account\n",
        "    token = {\"username\":\"pollendo\",\"key\":\"8666602a92bda984143ce79eca66ae75\"}\n",
        "    with open(\".kaggle/kaggle.json\", \"w\") as file:\n",
        "        json.dump(token, file)\n",
        "    !kaggle config set -n path -v/content\n",
        "\n",
        "    !cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json\n",
        "    !chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "    # download\n",
        "    !kaggle competitions download -c vu-dmt-2assignment\n",
        "    # unzip\n",
        "    !unzip training_set_VU_DM.csv\n",
        "    !unzip test_set_VU_DM.csv\n",
        "    !unzip submission_sample.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0nFwZkIb3ce",
        "colab_type": "text"
      },
      "source": [
        "# Read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_1SI4cfb3cf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set these global variables to switch between local validation or Kaggle submission\n",
        "DATA_FRACTION = 0.15\n",
        "\n",
        "TESTING = False\n",
        "\n",
        "\n",
        "if TESTING:\n",
        "    DATA_FRACTION = 1.0\n",
        "    with open(\"test_set_VU_DM.csv\",'r') as f_test:\n",
        "        test_rows = sum(1 for row in f_test)\n",
        "    df_test = pd.read_csv('test_set_VU_DM.csv', nrows=round(DATA_FRACTION*test_rows))\n",
        "\n",
        "\n",
        "with open(\"training_set_VU_DM.csv\",'r') as f_train:\n",
        "    train_rows = sum(1 for row in f_train)\n",
        "\n",
        "df = pd.read_csv('training_set_VU_DM.csv', nrows=round(DATA_FRACTION*train_rows))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ73axmEb3ci",
        "colab_type": "text"
      },
      "source": [
        "# Feature Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQyAG_-kb3cj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################ Normalize df per feat #################\n",
        "def norm(df, feature):\n",
        "    # Create x, where x the 'scores' column's values as floats\n",
        "    x = df[[feature]].values.astype(float)\n",
        "\n",
        "    # Create a minimum and maximum processor object\n",
        "    min_max_scaler = preprocessing.MinMaxScaler()\n",
        "\n",
        "    # Create an object to transform the data to fit minmax processor\n",
        "    x_scaled = min_max_scaler.fit_transform(x)\n",
        "\n",
        "    # Run the normalizer on the dataframe\n",
        "    df[feature] = x_scaled\n",
        "    \n",
        "    return df\n",
        "  \n",
        "\n",
        "def norm_feat(data, feature):\n",
        "    d = {'srch_id': data.srch_id.values, feature: data[feature].values}\n",
        "    df = pd.DataFrame(data=d)\n",
        "    min_ = df\n",
        "    max_ = df\n",
        "\n",
        "    min_ = min_.rename(columns={feature: 'min_'})\n",
        "    max_ = max_.rename(columns={feature: 'max_'})\n",
        "\n",
        "    min_ = min_.groupby('srch_id', as_index=False).min()\n",
        "    max_ = max_.groupby('srch_id', as_index=False).max()\n",
        "\n",
        "    df = df.merge(min_, on='srch_id', how='left')\n",
        "    df = df.merge(max_, on='srch_id', how='left')\n",
        "\n",
        "\n",
        "    data[feature] = (df[feature] - df.min_)/(df.max_ - df.min_)\n",
        "    \n",
        "    return data\n",
        "\n",
        "    \n",
        "\n",
        "###################### TARGET #########################\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def get_target(df):\n",
        "    df['target'] = 0.8 * 0.5 * (df.booking_bool.astype(float) + df.click_bool.astype(float)) \\\n",
        "        + 0.2 * sigmoid(np.log(df.position.astype(float))*df.click_bool.astype(float)\n",
        "                 + (1/df.position.astype(float))*(1-df.click_bool.astype(float)))\n",
        "    \n",
        "    df = norm(df, 'target')\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "####################### Time ##########################\n",
        "def get_time_feat(df):\n",
        "    df.insert(2, 'time', df.date_time.dt.time)\n",
        "    df.insert(3, 'weekday', df.date_time.dt.dayofweek)\n",
        "    df.insert(4, 'month', df.date_time.dt.month)\n",
        "    df.date_time = df.date_time.dt.date\n",
        "    df = df.drop(['date_time'], axis=1)\n",
        "    df = df.drop(['time'], axis=1)   \n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "################### Per search score ###################\n",
        "def ratio(data, feature, new_feat_name, inv=True):\n",
        "\n",
        "    d = {'srch_id': data.srch_id.values, feature: data[feature].values}\n",
        "    df = pd.DataFrame(data=d)\n",
        "    \n",
        "    avg_feat_per_scrch = df\n",
        "    avg_feat_per_scrch = avg_feat_per_scrch.rename(columns={feature: 'median_feat'})\n",
        "    avg_feat_per_scrch = avg_feat_per_scrch.groupby('srch_id', as_index=False).median()\n",
        "    \n",
        "    df = df.merge(avg_feat_per_scrch, on='srch_id', how='left')\n",
        "\n",
        "    if not inv:\n",
        "        data[new_feat_name] = df.median_feat / (df[feature] + 1)\n",
        "    else:\n",
        "        data[new_feat_name] = df[feature] / ( df.median_feat + 1)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def get_hotelworth(data):\n",
        "        data = ratio(data, 'price_usd', 'diff_price_per_srch', False)\n",
        "        data = ratio(data, 'prop_starrating', 'diff_star1_per_srch') \n",
        "        data = ratio(data, 'prop_review_score', 'diff_star2_per_srch')\n",
        "        data = ratio(data, 'prop_location_score1', 'diff_loc1_per_srch')\n",
        "        data = ratio(data, 'prop_location_score2', 'diff_loc2_per_srch')\n",
        "\n",
        "        data = norm_feat(data, 'diff_star1_per_srch')\n",
        "        data = norm_feat(data, 'diff_star2_per_srch')\n",
        "        data = norm_feat(data, 'diff_loc2_per_srch')\n",
        "        data = norm_feat(data, 'diff_loc1_per_srch')\n",
        "        data = norm_feat(data, 'diff_price_per_srch')\n",
        "        \n",
        "        # DO NOT REMOVE THESE: they will make the 'df_hotel_worth' with nan values\n",
        "        data.diff_star1_per_srch = data.diff_star1_per_srch.fillna(0)\n",
        "        data.diff_star2_per_srch = data.diff_star2_per_srch.fillna(0)\n",
        "        data.diff_loc1_per_srch = data.diff_loc1_per_srch.fillna(0)\n",
        "        data.diff_loc2_per_srch = data.diff_loc2_per_srch.fillna(0)\n",
        "        data.diff_price_per_srch = data.diff_price_per_srch.fillna(0)\n",
        "\n",
        "        df_hotel_worth = 0.33*(0.8*data['diff_star1_per_srch'] + 0.2*data['diff_star2_per_srch']) \\\n",
        "                        + 0.33*(0.9*data['diff_loc2_per_srch'] + 0.1*data['diff_loc1_per_srch'])  \\\n",
        "                        + 0.33*(data['diff_price_per_srch'])\n",
        "\n",
        "\n",
        "\n",
        "#         df_hotel_worth = 0.3*(0.8*data['diff_star1_per_srch'] + 0.2*data['diff_star2_per_srch']) \\\n",
        "#         + 0.3*(0.9*data['diff_loc2_per_srch'] + 0.1*data['diff_loc1_per_srch'])  \\\n",
        "#         + 0.1*(np.log(data['diff_price_per_srch']))\n",
        "               \n",
        "        data['hotel_worth'] = df_hotel_worth / 5\n",
        "        \n",
        "        data = norm_feat(data, 'hotel_worth')\n",
        "\n",
        "\n",
        "        return data\n",
        "    \n",
        "\n",
        "############# User's Preferences score #################\n",
        "def gaussian(x, mu, sig):\n",
        "    return np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.)))\n",
        "\n",
        "def per_cent_avg_std(df, category1, category2):\n",
        "    \"\"\"\n",
        "    NOTE: category1 has to be the user's entry\n",
        "    \n",
        "    category1: str of feature\n",
        "    category2: str of feature\n",
        "    \n",
        "    \"\"\"\n",
        "    # Get a df of the two categories only\n",
        "    diff_df = df[[category1, category2]]\n",
        "    \n",
        "    # Clean NaNs\n",
        "    diff_df = diff_df[diff_df[category1].notnull()]\n",
        "    diff_df = diff_df[diff_df[category2].notnull()]\n",
        "\n",
        "    # Get the diff on % scale\n",
        "    diff_price = np.abs(diff_df[category1] - diff_df[category2])/diff_df[category1]\n",
        "\n",
        "    diff_price = diff_price[diff_price.values<10000]\n",
        "\n",
        "    # Get the mean\n",
        "    avg_std = diff_price.median()\n",
        "    \n",
        "    return avg_std\n",
        "\n",
        "def diff_score(df, customer_feature, hotel_feature):\n",
        "    \"\"\"\n",
        "    float [0,1]\n",
        "    \n",
        "    customer_feature: str of feature\n",
        "    hotel_feature: str of feature\n",
        "    \n",
        "    \n",
        "    \"\"\"\n",
        "    df_booked = df[df.booking_bool == 1]\n",
        "    avg_std = per_cent_avg_std(df_booked, customer_feature, hotel_feature)\n",
        "\n",
        "    score_ = gaussian(df[hotel_feature], df[customer_feature]-df[customer_feature]*avg_std/2 , df[customer_feature]*avg_std)\n",
        "    \n",
        "    return avg_std, score_\n",
        "\n",
        "def diff_score_test(df, customer_feature, hotel_feature, avg_std):\n",
        "    score_ = gaussian(df[hotel_feature], df[customer_feature]-df[customer_feature]*avg_std/2 , df[customer_feature]*avg_std)\n",
        "    return score_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uijj2AU0b3cm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add new features to the raw data set and create novel target value\n",
        "def gen_feat(df, target_, stds=None):\n",
        "    # Parse date_time\n",
        "    df['date_time'] = pd.to_datetime(df.date_time, format='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "    # Time features\n",
        "    df = get_time_feat(df)\n",
        "\n",
        "    # Total count of people\n",
        "    df['srch_total_count'] = df['srch_adults_count'] + df['srch_children_count']\n",
        "\n",
        "    # Per-srch hotel score\n",
        "    df = get_hotelworth(df)\n",
        "    \n",
        "    #Remove competitor columns\n",
        "#     comp_columns = ['comp1_rate', 'comp1_inv', 'comp1_rate_percent_diff', 'comp2_rate', 'comp2_inv', 'comp2_rate_percent_diff', 'comp3_rate',\n",
        "#      'comp3_inv', 'comp3_rate_percent_diff', 'comp4_rate', 'comp4_inv', 'comp4_rate_percent_diff', 'comp5_rate', 'comp5_inv', 'comp5_rate_percent_diff',\n",
        "#      'comp6_rate', 'comp6_inv', 'comp6_rate_percent_diff', 'comp7_rate', 'comp7_inv', 'comp7_rate_percent_diff', 'comp8_rate', 'comp8_inv', 'comp8_rate_percent_diff']\n",
        "\n",
        "#     df = df.drop(columns=comp_columns, axis=1, errors=\"ignore\")\n",
        "    \n",
        "    if target_:\n",
        "        df = get_target(df)\n",
        "        \n",
        "        # Get rid of all non-clicked searches\n",
        "        df.insert(2, 'nr_clicks', df.groupby(\"srch_id\")[\"click_bool\"].transform(\"sum\"))\n",
        "        df = df[df[\"nr_clicks\"] > 0].drop(\"nr_clicks\", axis=1)\n",
        "        \n",
        "        # User's preferences\n",
        "        std_price, df['price_user_score'] = diff_score(df, 'visitor_hist_adr_usd', 'price_usd')\n",
        "        std_star1, df['star1_user_score'] = diff_score(df, 'visitor_hist_starrating', 'prop_starrating')\n",
        "        std_star2, df['star2_user_score'] = diff_score(df, 'visitor_hist_starrating', 'prop_review_score')\n",
        " \n",
        "        stds = []\n",
        "        stds.append(std_price)\n",
        "        stds.append(std_star1)\n",
        "        stds.append(std_star2)\n",
        "\n",
        "        df['user_score_perf'] = (df.price_user_score + df.star1_user_score + df.star2_user_score) /3\n",
        "        \n",
        "        return stds, df\n",
        "\n",
        "    else:\n",
        "        df['price_user_score'] = diff_score_test(df, 'visitor_hist_adr_usd', 'price_usd', stds[0])\n",
        "        df['star1_user_score'] = diff_score_test(df, 'visitor_hist_starrating', 'prop_starrating', stds[1])\n",
        "        df['star2_user_score'] = diff_score_test(df, 'visitor_hist_starrating', 'prop_review_score', stds[2])\n",
        "        \n",
        "        # DO NOT REMOVE THESE: they will make the 'user_score_perf' with nan values\n",
        "        df.price_user_score = df.price_user_score.fillna(0)\n",
        "        df.star1_user_score = df.star1_user_score.fillna(0)\n",
        "        df.star2_user_score = df.star2_user_score.fillna(0)\n",
        "\n",
        "        df['user_score_perf'] = (df.price_user_score + df.star1_user_score + df.star2_user_score) /3\n",
        "        \n",
        "        return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5al8uikrcx-z",
        "colab_type": "text"
      },
      "source": [
        "# Normalizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLuqdHuhczlP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalizing(df, type_='log'):\n",
        "    ### 'weekday' and 'month' removed\n",
        "    categorical_features=[ 'weekday', 'month', 'site_id', 'visitor_location_country_id', \n",
        "                          'visitor_location_country_id', 'prop_id', 'prop_country_id',\n",
        "                          'prop_brand_bool', 'promotion_flag', 'position', 'srch_destination_id', 'gross_bookings_usd',\n",
        "                         'srch_booking_window', 'srch_saturday_night_bool', 'random_bool', 'booking_bool', 'click_bool']\n",
        "\n",
        "    every_column_non_categorical = [col for col in df.columns if col not in categorical_features and col not in ['srch_id']]\n",
        "\n",
        "    for feature in every_column_non_categorical:\n",
        "        if is_numeric_dtype(df[feature]):\n",
        "          if type_ == 'log':\n",
        "            # log1 normalization\n",
        "            df[feature] = np.log1p(df[feature])\n",
        "          else:\n",
        "            # [0,1] normalization\n",
        "            norm(df, feature)\n",
        "\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBZeiB6Bb3co",
        "colab_type": "text"
      },
      "source": [
        "# Remove outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p7Ftavmb3cp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find and remove outliers for each pre-defined feature\n",
        "def remove_outliers_tukey(input_data):\n",
        "    # Focus on all other features, because e.g. IDs and Booleans do not make sense for outlier detection\n",
        "    features_to_avoid = ['srch_id', 'weekday', 'month', 'site_id', 'visitor_location_country_id', 'prop_country_id',\n",
        "       'prop_id', 'prop_review_score', 'prop_location_score1', 'prop_location_score2',\n",
        "       'prop_log_historical_price', 'price_usd', 'promotion_flag',\n",
        "       'srch_destination_id', 'srch_saturday_night_bool', 'orig_destination_distance', 'random_bool', 'click_bool', 'booking_bool'],\n",
        "    \n",
        "    indices_to_remove = []\n",
        "    \n",
        "    # Build the boundaries first, then find and remove outliers or replace them with their boundary\n",
        "    for column in input_data.columns.values:\n",
        "        if column in features_to_avoid:\n",
        "            continue\n",
        "\n",
        "        if is_numeric_dtype(input_data[column]):\n",
        "\n",
        "            # Calculate the interquartile range\n",
        "            Q1 = input_data[column].quantile(0.25)\n",
        "            Q3 = input_data[column].quantile(0.75)\n",
        "            IQR = Q3 - Q1\n",
        "\n",
        "            if IQR == 0:\n",
        "                continue\n",
        "\n",
        "            # Calculate the maximum value and minimum values according to the Tukey rule\n",
        "            max_value = Q3 + 1.5 * IQR\n",
        "            min_value = Q1 - 1.5 * IQR\n",
        "\n",
        "            # Iteratively build a list of indices to be removed\n",
        "            # Do not remove in place, because that will change the DataFrame and influence subsequent interquartile ranges for other columns\n",
        "            indices_to_remove = indices_to_remove + list(input_data.loc[(input_data[column].notnull()) & ((input_data[column] < min_value) | (input_data[column] > max_value))].index)\n",
        "\n",
        "    #for index in indices_to_remove:\n",
        "    indices_to_remove = list(set(indices_to_remove))\n",
        "    output_data = pd.concat([input_data[input_data[\"target\"] >= 0.4], input_data[input_data[\"target\"] < 0.4].drop(index=indices_to_remove, errors=\"ignore\")], axis=0)\n",
        "    output_data = output_data.reset_index(drop=True)\n",
        "    return output_data\n",
        "\n",
        "\n",
        "## DBSCAN outlier detection\n",
        "def remove_outliers_dbscan(input_data):\n",
        "    \n",
        "    # Focus on all other features, because e.g. IDs and Booleans do not make sense for outlier detection\n",
        "    features_to_avoid = ['srch_id', 'weekday', 'month', 'site_id', 'visitor_location_country_id', 'prop_country_id',\n",
        "       'prop_id', 'prop_review_score', 'prop_location_score1', 'prop_location_score2',\n",
        "       'prop_log_historical_price', 'price_usd', 'promotion_flag',\n",
        "       'srch_destination_id', 'srch_saturday_night_bool', 'orig_destination_distance', 'random_bool', 'click_bool', 'booking_bool'],\n",
        "\n",
        "    # Build the boundaries first, then find and remove outliers or replace them with their boundary\n",
        "    for column in input_data.columns.values:\n",
        "        if column in features_to_avoid:\n",
        "            continue\n",
        "            \n",
        "        if is_numeric_dtype(input_data[column]):\n",
        "            model = DBSCAN(eps=0.63).fit(input_data[column])\n",
        "            labels = model.labels_\n",
        "            labels = [('anomaly' if x==-1 else 'normal') for x in labels]\n",
        "    return output_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zon1Xrcab3cr",
        "colab_type": "text"
      },
      "source": [
        "# Handle missing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRDGONm4b3cs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop all columns that have at least one NaN value\n",
        "def remove_nans(df):\n",
        "    # For speed, we drop NaN values\n",
        "    columns_with_na = df.columns[df.isnull().any()]\n",
        "    df = df.drop(columns_with_na, axis=1)\n",
        "    \n",
        "    return df\n",
        "  \n",
        "# Replace NaN and Inf values of all numeric columns\n",
        "def replace_nans(data, unique_nan_values):\n",
        "  \n",
        "    # features = get_features(with_target=False)\n",
        "    \n",
        "    data = data.replace([np.inf, -np.inf], np.nan)\n",
        "    \n",
        "    if len(unique_nan_values) != 1:\n",
        "        for column in list(data):\n",
        "            if is_numeric_dtype(data[column]):\n",
        "                data[column] = data[column].fillna(unique_nan_values[column])\n",
        "              \n",
        "    else:     # fill all NaN values with the same number -- replace_nans(df, [-999])\n",
        "\n",
        "        for column in list(data):\n",
        "            if is_numeric_dtype(data[column]):\n",
        "                data[column] = data[column].fillna(unique_nan_values[0])\n",
        "    \n",
        "    return data\n",
        "\n",
        "def replace_nans_worst_case(data):\n",
        "    data = data.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    for column in list(data):\n",
        "        if is_numeric_dtype(data[column]) and data[column].isnull().any():\n",
        "            data[column] = data[column].fillna(data[column].min())\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etsCI9y1b3cu",
        "colab_type": "text"
      },
      "source": [
        "# Balance the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dPOvqtnb3cv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Removes non-clicked rows randomly up to a final ratio of 50/50 for clicked to non-clicked\n",
        "def balance_data(df, print_=False):\n",
        "    click_indices = df[df.click_bool == 1].index\n",
        "    random_indices = np.random.choice(click_indices, len(df.loc[df.click_bool == 1]), replace=False)\n",
        "    click_sample = df.loc[random_indices]\n",
        "\n",
        "    not_click = df[df.click_bool == 0].index\n",
        "    random_indices = np.random.choice(not_click, sum(df['click_bool']), replace=False)\n",
        "    not_click_sample = df.loc[random_indices]\n",
        "\n",
        "    df_new = pd.concat([not_click_sample, click_sample], axis=0)\n",
        "\n",
        "    if print_:\n",
        "        print(\"Percentage of not click impressions: \", len(df_new[df_new.click_bool == 0])/len(df_new))\n",
        "        print(\"Percentage of click impression: \", len(df_new[df_new.click_bool == 1])/len(df_new))\n",
        "        print(\"Total number of records in resampled data: \", len(df_new))\n",
        "\n",
        "    df = df_new.sort_values(['srch_id'], ascending=[True])\n",
        "    df = df.reset_index(drop=True)\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHjcS4r8b3cy",
        "colab_type": "text"
      },
      "source": [
        "# Split data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bRAzGWmTb3cz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splits the data into a training and validation set\n",
        "def split_data(train, ratio=0.7):\n",
        "    \n",
        "    temp_train = copy.deepcopy(train)\n",
        "    \n",
        "    uniq_searches = temp_train.srch_id.unique().tolist()\n",
        "    train_id = random.sample(uniq_searches, round(len(uniq_searches)*ratio))\n",
        "    \n",
        "    val_id = list(set(uniq_searches) - set(train_id))\n",
        "    \n",
        "    train_set = temp_train[temp_train['srch_id'].isin(train_id)]\n",
        "    validation_set = temp_train[temp_train['srch_id'].isin(val_id)]\n",
        "    \n",
        "    return train_set, validation_set\n",
        "\n",
        "# Splits the data into features and targets\n",
        "def prepare_features_and_targets(raw_df, testing=False):\n",
        "    list_of_columns_to_exclude = ['position', 'click_bool', 'booking_bool', 'gross_bookings_usd', 'target']\n",
        "\n",
        "    if testing:\n",
        "        return raw_df.drop(list_of_columns_to_exclude, axis=1, errors='ignore')\n",
        "    \n",
        "    df_t = raw_df.target\n",
        "    df_x = raw_df.drop(list_of_columns_to_exclude, axis=1, errors='ignore')\n",
        "\n",
        "    return df_x, df_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7-o4aqcb3c2",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uZkXkjsb3c3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Trains a model on the given input and returns the predictions on the given validation set\n",
        "def regressor_fit(x_train, t_train, x_val, model_type, eval_on_train=False):\n",
        "    if model_type == \"XGBOOST\":\n",
        "        model = xgboost.XGBRegressor(colsample_bytree=0.55,\n",
        "               gamma=0,                 \n",
        "               learning_rate=0.001,\n",
        "               max_depth=20, #8\n",
        "               min_child_weight=1.5,\n",
        "               n_estimators=51,                                                                    \n",
        "               reg_alpha=0.55,\n",
        "               reg_lambda=0.75,\n",
        "               subsample=0.6,\n",
        "               seed=42) \n",
        "\n",
        "    elif model_type == \"RANDOMFOREST\":\n",
        "      model = RandomForestRegressor(n_estimators=51,min_samples_leaf=5,min_samples_split=3)\n",
        "    elif model_type == \"ADABOOST\":\n",
        "      model = AdaBoostRegressor(random_state=0, learning_rate=0.1, loss='square' ,n_estimators=65)\n",
        "\n",
        "    # fit\n",
        "    model.fit(x_train, t_train)\n",
        "    if eval_on_train:\n",
        "        y_train = model.predict(x_train)\n",
        "        errors = abs(y_train - t_train)\n",
        "        print('Mean Absolute Error on Train:', round(np.mean(errors), 2), 'degrees.')\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNkVWnE8b3c7",
        "colab_type": "text"
      },
      "source": [
        "# Ranking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_SkL788b3c9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Returns a dataframe that has been sorted with respect to predictions per srch_id\n",
        "def rank_df(data_to_be_ranked, predictions, validation=True):    \n",
        "    \n",
        "    df = data_to_be_ranked[['srch_id', 'prop_id']]\n",
        "    df.insert(2, 'predictions', predictions)\n",
        "    \n",
        "    if validation:\n",
        "        df.insert(3, 'target', data_to_be_ranked['target'])\n",
        "        df.insert(4, 'relevance', 4 * data_to_be_ranked['booking_bool'] + data_to_be_ranked['click_bool'])\n",
        "\n",
        "    # Sort them to get a ranking\n",
        "    df = df.sort_values(['srch_id', 'predictions'], ascending=[True, False])\n",
        "    \n",
        "    if not validation:\n",
        "        # Set file name\n",
        "        time = str(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\n",
        "        file_name = 'submission_' + time + '.csv'\n",
        "\n",
        "        df = df.drop(['predictions'],axis=1)\n",
        "        df.to_csv(file_name, header=True, index=False)\n",
        "\n",
        "    return df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCUpmmmyb3dA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_dcg(ranking, cut=5):\n",
        "    len_ = min(cut, len(ranking))\n",
        "    dcg = ranking[:len_]    \n",
        "    result = 0\n",
        "    for i in range(1, len_):\n",
        "        result += (dcg[i-1]) / math.log(i+1, 2)\n",
        "    return result\n",
        "\n",
        "def compute_ndcg(val_set, val_pred):\n",
        "    ranked_val_set = rank_df(val_set, val_pred, validation=True)\n",
        "    ndcgs = []\n",
        "    for idx in ranked_val_set.srch_id.unique():\n",
        "\n",
        "        our_ranking = ranked_val_set[ranked_val_set['srch_id'] == idx][\"relevance\"].values\n",
        "        our_dcg = compute_dcg(our_ranking)\n",
        "\n",
        "        best_ranking = sorted(our_ranking, reverse=True)\n",
        "        best_dcg = compute_dcg(best_ranking)\n",
        "        \n",
        "        if best_dcg > 0:\n",
        "            ndcg = our_dcg / best_dcg\n",
        "        else:\n",
        "            ndcg = 0\n",
        "\n",
        "        ndcgs.append(ndcg)\n",
        "    return sum(ndcgs) / len(ndcgs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF78h8GEcVNQ",
        "colab_type": "text"
      },
      "source": [
        "# Code execution\n",
        "\n",
        "\n",
        "Train model on the full train dataset and see predictions on validation. From 10 folds, save the best model and make predictions with it on Test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPmtyi0Q9XCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preprocessing\n",
        "stds, df = gen_feat(df, True)\n",
        "\n",
        "# Set unique values for each column globally for proper reuse in test set\n",
        "# unique_values = {}\n",
        "# for column in list(df):\n",
        "#     unique_values[column] = random.randint(-9999, -999)\n",
        "\n",
        "df = normalizing(df, type_='minmax')\n",
        "df = replace_nans_worst_case(df)\n",
        "#df = replace_nans(df, [-999])\n",
        "#df = remove_nans(df)\n",
        "\n",
        "if TESTING:\n",
        "    df_test = gen_feat(df_test, False, stds)\n",
        "    df_test = normalizing(df_test, type_='log')\n",
        "    df_test = replace_nans(df_test, [-999])\n",
        "    \n",
        "    train_set = balance_data(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "_Uhqcwyrb3dC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "MODEL_TYPE = \"XGBOOST\" ### \"RANDOMFOREST\", \"ADABOOST\", \"XGBOOST\"\n",
        "\n",
        "# Two cases: Validation or Testing!\n",
        "if not TESTING:\n",
        "    ndcg_cross_validation = []\n",
        "    folds = 2\n",
        "    \n",
        "    best_ndcg = 0\n",
        "    \n",
        "    for i in range(folds):\n",
        "      \n",
        "        \n",
        "        # Prepare training and validation for model\n",
        "        train_set, val_set = split_data(df, 0.95)\n",
        "                \n",
        "        train_set = balance_data(train_set)\n",
        "                \n",
        "        train_x, train_t = prepare_features_and_targets(train_set)\n",
        "        val_x, val_t = prepare_features_and_targets(val_set)\n",
        "\n",
        "        # Predict and rank validation set\n",
        "        model = regressor_fit(train_x, train_t, val_x, model_type=MODEL_TYPE)\n",
        "        \n",
        "        val_y = model.predict(val_x)\n",
        "\n",
        "        ranking = rank_df(val_set, val_y, validation=True)\n",
        "        ndcg = compute_ndcg(val_set, val_y)\n",
        "        ndcg_cross_validation.append(ndcg)\n",
        "        \n",
        "        # Use this print statement to figure out a good split ratio\n",
        "        print(\"\\nShapes\", train_set.shape, val_set.shape, ':', ndcg, '\\n')\n",
        "        \n",
        "        if best_ndcg<ndcg:\n",
        "            best_ndcg = ndcg\n",
        "            best_model = model\n",
        "            print('*New BEST model\\n')\n",
        "            \n",
        "            # save model\n",
        "            filename = MODEL_TYPE+'2_model.pkl'\n",
        "            joblib.dump(best_model, filename) \n",
        "\n",
        "\n",
        "    print(\"All NDCGs\", ndcg_cross_validation)\n",
        "    print(\"Mean:\", sum(ndcg_cross_validation) / len(ndcg_cross_validation))\n",
        "\n",
        "else:\n",
        "\n",
        "    # Prepare training and validation for model\n",
        "    train_x, train_t = prepare_features_and_targets(train_set)\n",
        "    test_x = prepare_features_and_targets(df_test, testing=True)\n",
        "\n",
        "    # Predict and rank validation set\n",
        "    model = regressor_fit(train_x, train_t, test_x, model_type=MODEL_TYPE)\n",
        "    \n",
        "    test_y = model.predict(test_x)\n",
        "\n",
        "    ranking = rank_df(test_x, test_y, validation=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68wUOyboeXSC",
        "colab_type": "text"
      },
      "source": [
        "# Test set\n",
        "\n",
        "Use the trained model of the validation to predict test set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7oi83X5wjhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test = pd.read_csv('test_set_VU_DM.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdBgJECneV7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stds = [0.2,0.2,0.2]\n",
        "df_test = gen_feat(df_test, False, stds=stds)\n",
        "df_test = normalizing(df_test, type_='log')\n",
        "df_test = replace_nans(df_test, [-999])\n",
        "    \n",
        "test_x = prepare_features_and_targets(df_test, testing=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR-5Yqo6tJcH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the pickle file\n",
        "best_model = joblib.load('XGBOOST_model.pkl') \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brlgKC0uzoUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_y = best_model.predict(test_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFH5PWfAyB0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ranking = rank_df(test_x, test_y, validation=False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}